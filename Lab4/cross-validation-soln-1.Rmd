---
title: 'Gov 2018: Lab 4 Cross Validation (Solutions)'
author:
- 'Adeline Lo'
date: 'Tuesday February 15, 2022'
output:
  pdf_document: default
  html_document: default
---

This lab on Ridge Regression and the Lasso in R comes from p. 251-255 of "Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.

# CV Ridge Regression and the Lasso


```{r,warnings=FALSE}
rm(list=ls())
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)
#set seed
lab.seed<-202202
```

We will use the `glmnet` package in order to perform ridge regression and
the lasso. The main function in this package is `glmnet()`, which can be used
to fit ridge regression models, lasso models, and more. This function has
slightly different syntax from other model-fitting functions that we have
encountered thus far in this book. In particular, we must pass in an $x$
matrix as well as a $y$ vector, and we do not use the $y \sim x$ syntax.

Before proceeding, let's first ensure that the missing values have
been removed from the data, as described in the previous lab.


```{r}
Hitters = na.omit(Hitters)
```

Execute a ridge regression and the lasso in order to predict `Salary` on
the `Hitters` data. 

Set up data:


```{r}
x = model.matrix(Salary~., Hitters)[,-1] # trim off the first column
                                         # leaving only the predictors
y = Hitters$Salary
```

The `model.matrix()` function is particularly useful for creating $x$; not only
does it produce a matrix corresponding to the 19 predictors but it also
automatically transforms any qualitative variables into dummy variables.
The latter property is important because `glmnet()` can only take numerical,
quantitative inputs.

## Question 1. Ridge Regression
The `glmnet()` function has an alpha argument that determines what type
of model is fit. If `alpha = 0` then a ridge regression model is fit, and if `alpha = 1`
then a lasso model is fit. Fit a ridge regression model on `x` and `y` using the grid of lambda values from below.


```{r}
grid = 10^seq(10, -2, length = 100)
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)
```

By default the `glmnet()` function performs ridge regression for an automatically
selected range of $\lambda$ values. However, here we have chosen to implement
the function over a grid of values ranging from $\lambda = 10^{10}$ to $\lambda = 10^{-2}$, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. 

As we will see, we can also compute model fits for a particular value of $\lambda$ that is not one of the original
grid values. Note that by default, the `glmnet()` function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument `standardize = FALSE`.

Associated with each value of $\lambda$ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by `coef()`. In this case, it is a $20 \times 100$ matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of $\lambda$).

```{r}
dim(coef(ridge_mod))
plot(ridge_mod)    # Draw plot of coefficients

# [https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf] Each curve corresponds to a variable. It shows the path of its coefficient against the l1-norm of the whole coefficient vector as lambda varies. The axis above indicates the number of nonzero coefficients at the current lambda, which is the effective degrees of freedom (df ) for the lasso. Users may also wish to annotate the curves: this can be done by setting label = TRUE in the plot command.
```

We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm, when a large value of $\lambda$ is used, as compared to when a small value of $\lambda$ is used. Set $\lambda$ to its 50th value. What are the coefficients at this value?
What's their $l_2$ norm (remove the intercept value)?


```{r}
ridge_mod$lambda[50] #Display 50th lambda value
coef(ridge_mod)[,50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm
```

In contrast, what are the coefficients when $\lambda$ is at its 60th value? Their $l_2$
norm? Note the much larger $l_2$ norm of the coefficients associated with this
smaller value of $\lambda$.


```{r}
ridge_mod$lambda[60] #Display 60th lambda value
coef(ridge_mod)[,60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm
```


Split the samples into a 80% training set and a 20% test set in order
to estimate the test error of ridge regression and the lasso.


```{r}
set.seed(lab.seed)
index<-sample(nrow(Hitters),replace=TRUE) # to permute the row index
Hitters<-Hitters[index,]
y<-y[index]

train = Hitters[1:round(nrow(Hitters)*0.8,0),]
test = Hitters[(round(nrow(Hitters)*0.8,0)+1):nrow(Hitters),]

x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]
y_train = train$Salary
y_test = test$Salary
```

Next we fit a ridge regression model on the training set, and evaluate
its MSE on the test set, using $\lambda = 40$. Note the use of the `predict()`
function again: get predictions for a test set. Make sure to use`newx` argument.


```{r}
set.seed(lab.seed)
ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, thresh = 1e-12)
ridge_pred = predict(ridge_mod, s = 40, newx = x_test)
mse1<-mean((ridge_pred - y_test)^2)
mse1
```

The test MSE is `r mse1`.

Instead of arbitrarily choosing $\lambda = 40$, it would be better to
use cross-validation to choose the tuning parameter $\lambda$. We can do this using
the built-in cross-validation function, `cv.glmnet()`. By default, the function
performs 10-fold cross-validation, though this can be changed using the
argument `nfolds`. Set folds to 10 and calculate the $\lambda$ that best minimizes the training MSE (`lambda.min` item in object returned from `cv.glmnet()`).


```{r}
set.seed(lab.seed)
cv.out = cv.glmnet(x_train, y_train, alpha = 0, nfolds=10) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
```

Therefore, we see that the value of $\lambda$ that results in the smallest cross-validation
error is `r bestlam`. 

Plot the MSE as a function of $\lambda$ by using `plot()` on our returned object from our call to `cv.glmnet`.

```{r}
plot(cv.out) # Draw plot of training MSE as a function of lambda
# This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the lambda sequence (error bars). Two special values along the lambda sequence are indicated by the vertical dotted lines. lambda.min is the value of lambda that gives minimum mean cross-validated error, while lambda.1se is the value of lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.
```

What is the test MSE associated with this value of $\lambda$ ($\lambda$ that best minimizes the training MSE)?

```{r}
set.seed(lab.seed)
ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mse2<-mean((ridge_pred - y_test)^2) # Calculate test MSE
mse2
```

This represents a further improvement over the test MSE that we got using
$\lambda = 40$. 

Refit the ridge regression model on the full data set, using the value of $\lambda$ chosen by cross-validation, and examine the coefficient
estimates.


```{r}
out = glmnet(x, y, alpha = 0) # Fit ridge regression model on full dataset
predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
```

As expected, none of the coefficients are exactly zero - ridge regression does not
perform variable selection!

## Question 2. The Lasso
We saw that ridge regression with a wise choice of $\lambda$ can outperform least
squares as well as the null model on the Hitters data set. We now ask
whether the lasso can yield either a more accurate or a more interpretable
model than ridge regression. Fit a lasso model, however, this time use the argument `alpha=1`.
Other than that change, proceed just as you did in fitting a ridge model:


```{r}
set.seed(lab.seed)
lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   lambda = grid) # Fit lasso model on training data

plot(lasso_mod)    # Draw plot of coefficients
```

Notice that in the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients are exactly equal to zero. 

Now perform cross-validation with folds equal to 10 and compute the associated test error:


```{r}
set.seed(lab.seed)
cv.out = cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mse3<-mean((lasso_pred - y_test)^2) # Calculate test MSE
mse3
```


## Question 3. K-fold cross validation

Conduct K-fold cross validation for the ridge and lasso, with $k\in\{5,7,9,11,13,15\}$. Assess the models at each $k$ value by calculating the risk/prediction error in the test set and suggest the best $k$ for each. Suggest the best final model.

```{r}
set.seed(lab.seed)
K<-seq(5,15,2)
mse<-data.frame(ridge=rep(NA,length(K)),lasso=rep(NA,length(K)))
blam.lasso <- blam.ridge <- rep(NA, length(K))
for(k in 1:length(K)){
  cv.lasso = cv.glmnet(x_train, y_train, alpha = 1, nfolds = K[k])
  cv.ridge = cv.glmnet(x_train, y_train, alpha = 0, nfolds = K[k])
  blam.lasso[k] <- cv.lasso$lambda.min
  blam.ridge[k] <- cv.ridge$lambda.min
  lasso_pred = predict(lasso_mod, s = cv.lasso$lambda.min, newx = x_test)
  ridge_pred = predict(ridge_mod, s = cv.ridge$lambda.min, newx = x_test) # Use best lambda to predict test data
  mse$ridge[k]<-mean((ridge_pred - y_test)^2) # Calculate test MSE
  mse$lasso[k]<-mean((lasso_pred - y_test)^2)
  cat("For k=",K[k],", MSE for ridge is = ", mse$ridge[k],"\n")
  cat("For k=",K[k],", MSE for lasso is = ", mse$lasso[k],"\n")
}
cat("Best ridge k is: ",K[which(mse$ridge==min(mse$ridge))],"\n")

cat("Best lasso k is: ",K[which(mse$lasso==min(mse$lasso))],"\n")

min(mse)

blam.lasso
blam.ridge
```

Best final model: lasso with k=5.